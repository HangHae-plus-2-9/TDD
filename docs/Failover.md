# 장애대응 훈련

## 장애대응 대표 유형

### 1. 트래픽 부하

- 서버 사양: 0.25 vCPU, 0.5 GB Memory, 2 instance

- Artillery 활용
  - ./artillery 폴더 내 `artillery.yml` 파일 참고
  - GET 상품 조회 api
    - Parameterized API call: 30 RPS

### 2. 외부서비스

- payment mock 서버 활용
  - Up 상태: payment endpoint 200 OK
  - Down 상태: payment endpoint 500 Internal Server Error

- 상기 서버가 Up인 경우
  - 서버 정상 작동
- 상기 서버가 Down인 경우
  - create payment endpoint 500 반환, 에러로그 발생
  - 에러로그가 5분 내에 30회 이상 발생 시 서버 Down으로 간주
    - 개발자에게 슬랙 알람 발송

### 3. 휴먼 에러 - 비즈니스 로직 상 실수

- 관리자에 의한 DB table drop
- 미숙한 예외 처리로 인한 특수한 에러 발생

### 4. 배포 시 Migration Error

- Migration을 고려하지 않은 column 변경
  - DB서버, nest서버에서 순차적으로 배포가 이루어지지 않을 경우에 발생하는 상황

### 5. 코드 에러

- 제대로된 비동기 처리가 되지 않아 코드상에서 에러가 발생한 경우

## 장애대응 시나리오

> 1. **장애 탐지** (시스템 알람, 고객센터, QA팀, 사내제보)
  - 결제 시스템 상에서 500에러 발생으로 인해 결제 모듈이 정상적으로 작동하지 않음
  - 결제 서버의 다운
> 2. **장애 공지** (Slack, KakaoTalk 등 실시간 알람을 받을 수 있는 SNS > 활용)
  - Error단계의 log가 cloudWatch로 전송, 에러인식 후 AWS의 SNS를 에서 Slack으로 현재 상황 전파, 알림
> 3. **장애 전파** (사내 내부 조직, 팀원, 고객 등)
  - slack 알람으로 인지한 장애를 개발팀, 고객 대응팀 톡방에 전파
  - 왜 이런 장애가 일어났는지 파악 후 고객 대응팀에 상황 전파
> 4. **장애 복구**
  - 결제 단계부터 역추적으로 들어가서 DB에 등록되어 있는 상품의 금액 데이터가 소실 되었다는 것을 발견
  - RDS의 스냅샷으로 새로운 DB를 만들어서 소실된 DB와, 이후 저장된 데이터들을 합친 후 새로운 DB를 서버에 연결
> 5. **장애 후속 조치**
  - 복구 후 stage 단계에서 결제 프로세스 재 점검후 hoxfix로 급하게 배포
> 6. **장애 회고** (왜 장애가 발생했으며, 앞으로 어떻게 처리해야 할지 대응책 마련)
  - 왜 DB에 직접적으로 SQL문을 입력을 해야 했는가?
  - 장애가 인지된 후에 복구까지 걸린시간을 통해 제대로 잘 응대 했는가?
  - 고객응대 팀에서 제대로 대응을 할 수 있도록 지금 일어난 일을 제대로 설명 했는가?
  - 다시 이런 일이 일어날 일이 있는가?
  - 만약 재발생한 경우 누구든지 대응할 수 있는가?
